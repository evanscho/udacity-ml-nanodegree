{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "What is machine learning?\n",
    "\n",
    "\"In the world there are humans and machines. Humans learn from past experiences. Machines need to be programmed. Machine learning is programming a machine to learn from past experiences. And past experiences for machines are *data*\"\n",
    "\n",
    "# Decision Trees\n",
    "\n",
    "Decision trees are a series of questions you ask at one's data to come to a decision. In the example, we built the questions about the features that had a nice split over the data. The data is below, and so is the compiled decision tree.\n",
    "\n",
    "## Data\n",
    "\n",
    "| Gender | Age | App Downloaded|\n",
    "|--------|-----|---------------|\n",
    "| F | 15 | Pokemon Go |\n",
    "| M | 12 | Pokemon Go |\n",
    "| F | 25 | WhatsApp |\n",
    "| M | 32 | Snapchat |\n",
    "| F | 40 | WhatsApp |\n",
    "| M | 14 | Pokemon Go |\n",
    "\n",
    "## Decision tree\n",
    "![Decision Tree](images/decision-tree.png)\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes classifier is probabilistic classifier that assumes that features observed are independent from each other, that is, there are no correlation between feature observed.\n",
    "\n",
    "For example: suppose we're building a spam detector and we have a data set of 100 e-mails, 25 flagged as spam and 75 not spam. The word `cheap` appears in 20 out of the 25 spam emails, and only in 5 out of the 75 non-spam e-mails. From this, we can conclude that there is a 80% chance of an e-mail being spam if it contains the word `cheap`.\n",
    "\n",
    "**Remark:** If the number of features tend to infinity, the Naive Bayes classifier will converge to 0, because the probability of features will always be < 1. Also, if a feature probability happens to be 0, it will wipe out all information being calculated.\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "For each step, we evaluate all possible next steps and select the step that will maximize our current position towards our goal.\n",
    "\n",
    "Example: while descending a mountain, we evaluate all next steps to choose the step that will take us closer to the base of the mountain, and so on, until we get to the base of the mountain.\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "We plot our data into a chart and draw a line that best fits all the points we plotted. The best fit is the line which minimizes the error. In this case, the error is the sum of the distances between the points and the line. To do this, we can use two techniques: Gradient Descent (has to deal with negative distances) and Least Squares (doesn't have to deal with negative distances - *real life usage*).\n",
    "\n",
    "Example: assuming house prices are related to their square footage, estimate a house price given its square footage and 2 others houses prices, and square footage.\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "We plot out data to a graph and each data-point. We then draw a line that best separate the two sets of points. This is also done using gradient descent. The error function is called log loss, and is correlated to the misclassification and the proximity of points to the separation line.\n",
    "\n",
    "Example: students previous grade and test grade for acceptance at a college. Given the two grades of a sample student and a data-set, we have to plot the sample data onto the graph and check if it is inside the acceptance region or not.\n",
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "Given all possible lines that split a data set:\n",
    "1. we identify the points close to these lines\n",
    "2. then we discard the other points\n",
    "3. we calculate the distances of the remaining points to each line\n",
    "4. then we evaluate the minimal distances and select the line that has the largest minimal distance\n",
    "\n",
    "To maximize the minimal distance we use gradient descent. The name Support Vector Machine is because the points close to the lines are called the \"support\".\n",
    "\n",
    "Example:\n",
    "\n",
    "![Example](images/support-vector-machines.png \"Example\")\n",
    "\n",
    "Comparision:\n",
    "\n",
    "![Comparison](images/logistic-regression-and-svms.png)\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "In a simple example, the instructor defined a neural network as a combination of multiple evaluations using previous known methods for splitting data set. His example basically consisted of independent nodes that plot their inputs into graphs and check if they are above or below some separating lines.\n",
    "\n",
    "![Neural Network Questions](images/neural-network-questions.png \"Neural Network Questions\")\n",
    "\n",
    "![Neural Network Topography](images/neural-network-topography-2.png \"Neural Network Topography\")\n",
    "\n",
    "Why called \"neural network\"? These nodes resemble the brain's neurons, which take as imput the output of other neurons in the form of nervous impulses, and decide to fire a nervous impulse or not.\n",
    "\n",
    "# Kernel Method\n",
    "\n",
    "This method consists of identifying an auxiliary function that allow us to split the data, based on the auxiliary function. In a 2-D example, we had 4 points `red(0,3) green(1,2) green(2,1) red(3,0)`. From these points it was easy to identify the function f(x,y) = xy, as a function which separates the green and red points (since will make (1,2) and (2,1) > 0, and (0,3) and (3,0) < 0. Which then gave us the table:\n",
    "\n",
    "|x|y|xy|\n",
    "|---|---|---|\n",
    "|0|3|0|\n",
    "|1|2|2|\n",
    "|2|1|2|\n",
    "|3|0|0|\n",
    "\n",
    "It is clear from the table that `xy = 1` splits the data in two sets.\n",
    "\n",
    "We can then represent this data as curves or as planes. When representing as planes, we're creating a third variable z to represent the function we chose; in effect we're moving one group upwards and one downwards on the z axis.\n",
    "```\n",
    "xy = 0 -> red\n",
    "xy = 1 -> frontier\n",
    "xy = 2 -> green\n",
    "```\n",
    "\n",
    "![Kernel Trick Planes](images/kernel-trick-plane.png)\n",
    "\n",
    "![Kernel Trick Curves](images/kernel-trick-curve.png)\n",
    "\n",
    "# K-means clustering\n",
    "\n",
    "This algorithm consists of:\n",
    "\n",
    "1. Given N Clusters:\n",
    "2. Plot N random control points into the data set.\n",
    "3. Mark all the points to match the control point that is closest to them.\n",
    "4. Move the control points to the center of their marked points.\n",
    "5. If a control point was moved, then repeat 3, 4 and 5.\n",
    "\n",
    "![K-means Clustering](images/k-means-clustering.png)\n",
    "\n",
    "# Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering is an algorithm used when we don't know the number of clusters. It consists of:\n",
    "\n",
    "1. Given a chosen maximum distance that will be allowed between 2 points to cluster them:\n",
    "2. Select the 2 points that are closest to each other\n",
    "3. If the distance between them is above the max distance, stop.\n",
    "4. Else, group them in a cluster\n",
    "5. Repeat 2 to 5.\n",
    "\n",
    "![Hierarchical Clustering](images/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
